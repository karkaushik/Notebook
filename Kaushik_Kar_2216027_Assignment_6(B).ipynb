{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Deep Convolution Neural Network**\n",
        "\n",
        "Deep convolutional neural network has recently been applied to image classification with large image datasets. A deep CNN is able to learn basic filters automatically and combine them hierarchically to enable the description of latent concepts for pattern recognition.\n",
        "\n",
        "\n",
        "MNIST is a classic dataset of handwritten images has served as the basis for benchmarking classification algorithms.\n",
        "\n",
        "Our goal here is to train the machine with humongous no. of images of handwritten digits and evaluating how well **Convolution Neural Network** can classify them correctly.\n"
      ],
      "metadata": {
        "id": "EmcK-6T_Uwn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "b0G3hNlpMd29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8e-mdtkoQhvB"
      },
      "outputs": [],
      "source": [
        "import numpy as np                 # advanced math library for linear algebra\n",
        "import matplotlib.pyplot as plt    # MATLAB like plotting, routines for data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random                      # For generating random number  \n",
        "%matplotlib inline\n",
        "#MNIST dataset is included in keras\n",
        "from keras.datasets import mnist   #Importing the dataset   \n",
        "\n",
        "#A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
        "from keras.models import Sequential   \n",
        "\n",
        "from keras.layers.core import Dense, Dropout, Activation #types of layers to be build\n",
        "from keras.utils import np_utils #numpy realted tools, here used for one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D,GlobalAveragePooling2D,Flatten \n",
        "from keras.layers.normalization.batch_normalization import BatchNormalization"
      ],
      "metadata": {
        "id": "PdAlxCCwREkY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **ImageDataGenerator**- Keras ImageDataGenerator is used for getting the input of the original data and further, it makes the transformation of this data on a random basis and gives the output resultant containing only the data that is newly transformed.\n",
        "* **Conv2D**- Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
        "* **Max pooling**- It downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window for each channel of the input. \n",
        "* **ZeroPadding2D**- when moving the feature detector across the input image all the pixels except the edge pixels are taken for calculation repeatedly. So there is unfairness and to address this there is a process called Padding (adding a frame). This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.\n",
        "* **GlobalAveragePooling2D**- globalAveragePooling2d() function is used for applying global average pooling operation for spatial data.\n",
        "* **Flatten** - Flatten layer is used to make the multidimensional input one-dimensional, commonly used in the transition from the convolution layer to the full connected layer.\n",
        "* **BatchNormalization**- It allows each layer of a network to learn by itself a little bit more independently of other layers. Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n"
      ],
      "metadata": {
        "id": "gtEQzDgWOVyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading and Splitting the Dataset**"
      ],
      "metadata": {
        "id": "xiZl6zECUf7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST dataset has total 70000 data is splitted in 60000 (28x28 px) dataset is used as training images and 10000(28x28 px) data is used as test images."
      ],
      "metadata": {
        "id": "ez-CSeWKUiCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test)=mnist.load_data()#loading the dataset and splitting in training and test set\n",
        "#printing the shape of the training and test set\n",
        "print(\"X_train_shape\", X_train.shape)  #printing the x_train shape\n",
        "print(\"X_test_shape\", X_test.shape)     #printing the x_test shape\n",
        "print(\"y_train_shape\", y_train.shape)   #printing the y_train shape\n",
        "print(\"y_test_shape\", y_test.shape)     #printing the y_test shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocecqibMS_uY",
        "outputId": "d709853e-b609-43e2-b49a-a8da6ee38b20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "X_train_shape (60000, 28, 28)\n",
            "X_test_shape (10000, 28, 28)\n",
            "y_train_shape (60000,)\n",
            "y_test_shape (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data (Image) Processing**"
      ],
      "metadata": {
        "id": "xnrie2P4-0Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train=X_train.reshape(60000,28,28,1) # here we taking imahes in greyscale so we are taking 1 channel.\n",
        "#X_train=X_train.reshape(60000,28,28,3) if we want to consider RGB then thre should be 3 channel\n",
        "X_test=X_test.reshape(10000,28,28,1)\n",
        "#Before applying any normalization, the type of the entire dataframe must be casted from 'str' to 'float' as far as it contains only numerical values.\n",
        "X_train=X_train.astype('float32')#changing the data to 32 bit float\n",
        "X_test=X_test.astype('float32')#changing the data to 32 bit float\n",
        "\n"
      ],
      "metadata": {
        "id": "uD-TY-EAUltH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalization**\n",
        "We will follow min-max standarization (since pixel cant take negative values so we cant us standard scalar process).\n",
        "We want to normalize the input data in range 0-1 rather than 0-255.\n",
        "\n",
        "**Data normalization** is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution. This makes convergence faster while training the network."
      ],
      "metadata": {
        "id": "SOFUXLC2-sAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train /= 255   #1byte = 8 bits. (Image is a 2D array) 2^8 =256, so 256 different values can be fit into 1 (0-255)\n",
        "X_test /= 255   # normalize each value for each pixel "
      ],
      "metadata": {
        "id": "bgdmJ-zOWuDh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of One Hot Encoding**\n",
        "\n",
        "It is a part of Data Preprocessing. In Deep Learning model to feed the model with data that enables it to make a classification decision, we would require to perform One Hot Encoding during data processing."
      ],
      "metadata": {
        "id": "AcCNS5izJ5kO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = 10 # total number of unique digits is 10,i.e., 0-9\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, nb_classes) #it is used to convert array of labeled data(from 0 to nb_classes - 1 ) to one-hot vector. \n",
        "Y_test = np_utils.to_categorical(y_test, nb_classes)\n"
      ],
      "metadata": {
        "id": "KnI0lG1zUzPZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Model Type: Sequential**\n",
        "\n",
        "There are three ways to create Keras models:\n",
        "Sequential Model, Functional API, Model Subclassing \n",
        "\n",
        "The **Sequential Model** in Keras allows us to create models layer-by-layer for most problems. It is limited to single-input, single-output stacks of layers.\n",
        "\n",
        "The Sequential model API is a way of creating deep learning models where an instance of the Sequential class is created and model layers are created and added to it."
      ],
      "metadata": {
        "id": "Kl7aklg4y89R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()"
      ],
      "metadata": {
        "id": "2Fg4kpilU6A6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding Convolution Layer\n",
        "\n",
        "The convolutional layer is considered an essential block of the CNN.\n",
        "\n",
        "A convolutional layer contains a set of filters whose parameters need to be learned. The height and weight of the filters are smaller than those of the input volume.\n",
        "\n",
        "Input shape is 28x28x1 as we are taking input 3D data."
      ],
      "metadata": {
        "id": "5iB-VZ0NTHcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convolution Layer 1 is of 32 filters each of which have 3x3 matrices of feature map \n",
        "model.add(Conv2D(32,(3,3),input_shape=(28,28,1)))\n",
        "#Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n",
        "#Input_shape - As we are taking input 3D data so the shape  is 28x28x1"
      ],
      "metadata": {
        "id": "i_TgtMY1U9YY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Batch Normalization**\n",
        "* Batch normalization is a technique for training very deep neural networks that normalizes the contributions to a layer for every mini-batch. \n",
        "* This has the impact of settling the learning process and drastically decreasing the number of training epochs required to train deep neural networks. \n",
        "\n"
      ],
      "metadata": {
        "id": "3RILjzEopHsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(BatchNormalization(axis=-1))"
      ],
      "metadata": {
        "id": "K-ry_cpXVTT8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RELU Activation Function**\n",
        "\n",
        "1. Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it.\n",
        "\n",
        "2. The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
        "\n",
        "3. Rectifier function (RELU) – max(X,0) – take the maximum, anything positive keep that as it is but anything negative make it 0.\n",
        "\n",
        "4. It is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
        "\n",
        "5. RELU is such an actiavtion function that preserves the property of linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned."
      ],
      "metadata": {
        "id": "ekMIwquMTWF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iGTlR8EaTV0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convLayer01 = Activation('relu')"
      ],
      "metadata": {
        "id": "85FIsr7oVZ9r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding Second Convolution Layer**\n",
        "Convolution Layer 2 is of 32 filters each of which have 3x3 matrices of feature map."
      ],
      "metadata": {
        "id": "9iDkG0MGZFQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ConvolutionLayer2\n",
        "model.add(Conv2D(32,(3,3)))#here the input_shape is not required because this layers can do automatic shape i\n",
        "model.add(BatchNormalization(axis=-1))"
      ],
      "metadata": {
        "id": "TNJqJZ0FVg6u"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Max Pooling**\n",
        "* We need to pool the most prominent feature from the feature map. This is to be done by Max Pooling.\n",
        "* It is a pooling operation that selects the maximum element from the region of the feature map covered by the filter(2x2 matrix usually). \n",
        "* The output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map."
      ],
      "metadata": {
        "id": "QJYG6cMKf9yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convLayer02=MaxPooling2D(pool_size=(2,2))#Max Pooling to reduce the spatial dimensions of the output volume\n",
        "#pool size is the 2x2 matrix\n",
        "#We need to pool the most prominent feature from the feature map. This is to be done by Max Pooling."
      ],
      "metadata": {
        "id": "mAf8Ob_GVzs3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(convLayer02)"
      ],
      "metadata": {
        "id": "LEz4LSZOV8Vp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding Third Convolution Layer**"
      ],
      "metadata": {
        "id": "zoueanPYZzyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convolutionLayer3\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "convLayer03=Activation('relu')\n",
        "model.add(convLayer03)"
      ],
      "metadata": {
        "id": "1M8Vi7D0WDyj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding Fourth Convolution Layer**"
      ],
      "metadata": {
        "id": "jKxQlTeNiiki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convolutionLayer4\n",
        "model.add(Conv2D(64,(3,3)))\n",
        "model.add(BatchNormalization(axis=-1))\n",
        "convLayer03=Activation('relu')\n",
        "convLayer04=MaxPooling2D(pool_size=(2,2))\n",
        "model.add(convLayer04)\n",
        "model.add(Flatten()) #flatenning the pooled feature map "
      ],
      "metadata": {
        "id": "Us4ss2qLWhze"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fully Connected Layer**\n",
        "* Fully connected layers are global (they can introduce any kind of dependence).\n",
        "* Fully connected layers are an essential component of Convolutional Neural Networks (CNNs), which have been proven very successful in recognizing and classifying images for computer vision. \n"
      ],
      "metadata": {
        "id": "JyekiDBnjRqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fully Connected Layer 5\n",
        "#Dense Layer is simple layer of neurons in which each neuron receives input from all the neurons of previous layer\n",
        "model.add(Dense(512))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))"
      ],
      "metadata": {
        "id": "B6Cg_A_AW7Rm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fully Connected layer 6\n",
        "model.add(Dropout(0.2))#to prevent the net from overfitting a dropout layer ignores a set of neurons randomly\n",
        "model.add(Dense(10))#Dense Layer is simple layer of neurons in which each neuron receives input from all the neurons of previous layer\n",
        "model.add(Activation('softmax'))\n",
        "#The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution.\n",
        "#softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.\n",
        "#The softmax activation function transforms the raw outputs of the neural network into a vector of probabilities, essentially a probability distribution over the input classes."
      ],
      "metadata": {
        "id": "1tKMVXCQXPn3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summarize Model**\n",
        " Keras provides a way to summarize a model. The summary is textual and includes information about:\n",
        "\n",
        "      * The layers and their order in the model.\n",
        "      * The output shape of each layer.\n",
        "      * The number of parameters (weights) in each layer.\n",
        "      * The total number of parameters (weights) in the model.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b8AaCN6SfuuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEuPmsceXuJm",
        "outputId": "0ebf3376-368a-449c-e2d8-a484683119ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 26, 26, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 24, 24, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 10, 10, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 10, 10, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 8, 8, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               524800    \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 512)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                5130      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 597,738\n",
            "Trainable params: 596,330\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Trainable params** - .\n",
        "Trainable Parameters in general are weights that are learnt during training. They are weight matrices that contribute to model’s predictive power, changed during back-propagation process. \n",
        "### **Non-trainable params** -\n",
        "Non-trainable parameters is the number of weights that are not updated during training with backpropagation.\n",
        "There are mainly two types of non-trainable weights:\n",
        "\n",
        "* The ones that you have chosen to keep constant when training. This means that keras won't update these weights during training at all.\n",
        "* The ones that work like statistics in BatchNormalization layers. They're updated with mean and variance, but they're not \"trained with backpropagation\".\n",
        "\n",
        "Here, weights of all the parameters are  updated during training with backpropagation. "
      ],
      "metadata": {
        "id": "frE3jBbEgflY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compiling The Model**\n"
      ],
      "metadata": {
        "id": "UNz9tnEOkDrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The loss function**\n",
        "\n",
        "* It is a critical part of model training: it quantifies how well a model is performing a task by calculating a single number, the loss, from the model output and the desired target.\n",
        "\n",
        "\n",
        "* **Categorical crossentropy** is a loss function that is used in multi-class classification tasks. \n",
        "\n",
        "* These are tasks where an example can only belong to one out of many possible categories, and the model must decide which one.\n",
        "* The cross entropy is a measure of how different is the predicted distribution from the target distribution.\n",
        "\n",
        "### **Optimizer function** -\n",
        " * The optimizer helps to determine how quickly the model learns through gradient descent\n",
        " * The rate at which descends the gradient is called learning rate.\n",
        " * **Adam optimization** is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
        " \n",
        "###**Metric**\n",
        " * Calculates how often predictions equal labels.\n",
        "\n",
        "* This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true. This frequency is ultimately returned as binary accuracy.\n"
      ],
      "metadata": {
        "id": "b-OE2VbuZF8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "McxxfJybXwGc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Model**\n",
        "* We train the model by calling fit method.\n",
        "\n",
        "* During training machine takes in input and weights are decided by computer randomly.\n",
        "\n",
        "* Then it calculates the difference between the actual output and desired output, that we called the loss function. The optimizer function directs the machine how the weights should be adjusted.\n",
        "\n",
        "* The fit method refers this cycle of calcualte --> compare --> adjust.\n",
        "\n",
        "  * **width_shift_range** is a floating point number between 0.0 and 1.0 which specifies the upper bound of the fraction of the total width by which the image is to be randomly shifted, either towards the left or right.\n",
        "  * ImageDataGenerator class allows you to randomly **rotate images** through any degree between 0 and 360 by providing an integer value in the rotation_range argument.\n",
        "  * **Shear_range** specifies the angle of the slant in degrees. This creates a sort of 'stretch' in the image, which is not seen in rotation.\n",
        "  * Input ---> X_train\n",
        "  * Output --->  Y_train\n",
        "  * Number of times cycle should run ---> epochs\n",
        "  * Controlling the amount of o/p to be produced ---> verbose"
      ],
      "metadata": {
        "id": "6wqSJUMdz_6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data augmentation prevents overfitting by slightly changing the data randomly\n",
        "#Keras ImageDataGenerator was used to perform data augmentation.\n",
        "# Keras has a great built-in feature to do automatic augmentation\n",
        "gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08,\n",
        "                         shear_range=0.3,\n",
        "                         height_shift_range=0.08, zoom_range=0.08)\n",
        "test_gen = ImageDataGenerator()"
      ],
      "metadata": {
        "id": "pipkSoA9YE9w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator= gen.flow(X_train,Y_train,batch_size=128)\n",
        "test_generator= test_gen.flow(X_test,Y_test,batch_size=128)"
      ],
      "metadata": {
        "id": "xqnbAjiiYLAn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator,steps_per_epoch=60000//128,epochs=5,verbose=1,\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps=10000//128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QppYIREbY2It",
        "outputId": "e29a573d-d003-4326-aa67-bbee619118a7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "468/468 [==============================] - 199s 423ms/step - loss: 0.1579 - accuracy: 0.9514 - val_loss: 0.0938 - val_accuracy: 0.9709\n",
            "Epoch 2/5\n",
            "468/468 [==============================] - 193s 412ms/step - loss: 0.0604 - accuracy: 0.9816 - val_loss: 0.0275 - val_accuracy: 0.9919\n",
            "Epoch 3/5\n",
            "468/468 [==============================] - 194s 415ms/step - loss: 0.0493 - accuracy: 0.9845 - val_loss: 0.0878 - val_accuracy: 0.9700\n",
            "Epoch 4/5\n",
            "468/468 [==============================] - 196s 419ms/step - loss: 0.0411 - accuracy: 0.9872 - val_loss: 0.0239 - val_accuracy: 0.9921\n",
            "Epoch 5/5\n",
            "468/468 [==============================] - 196s 419ms/step - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.1191 - val_accuracy: 0.9646\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91c73d0590>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation of the Model**\n",
        "Evaluation is a process during development of the model to check whether the model is best fit for the given problem and corresponding data. "
      ],
      "metadata": {
        "id": "oTAo-U5t-sfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score=model.evaluate(X_test,Y_test)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwp1Q1OgbUp7",
        "outputId": "5873ee2d-d3f8-4735-dda6-9d6c8b67f337"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 8s 27ms/step - loss: 0.1189 - accuracy: 0.9647\n",
            "Test score: 0.11894373595714569\n",
            "Test accuracy: 0.9646999835968018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test Score/Loss Score**\n",
        "It is the sum of errors made for each example in training or validation sets. Loss value implies how poorly or well a model behaves after each iteration of optimization.\n",
        "\n",
        "Here we get test score is 0.029\n",
        "### **Test Accuracy**\n",
        "The accuracy of a model is usually determined after the model parameters and is calculated in the form of a percentage. It is the measure of how accurate your model's prediction is compared to the true data.\n",
        "\n",
        "Here we get test accuracy is 0.99"
      ],
      "metadata": {
        "id": "OcRi-cFHDN63"
      }
    }
  ]
}